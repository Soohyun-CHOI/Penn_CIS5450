{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whOFv2wpMlhb"
      },
      "source": [
        "# Homework 0: Getting Started (10 points)\n",
        "\n",
        "## Due: September 6th, 2023 by 10:00pm (with grace period)\n",
        "\n",
        "For this initial assignment, our primary goal is to familiarize you with the Jupyter/Python and Apache Spark \"software stack\" we will use through the semester.\n",
        "\n",
        "**This class assumes you are comfortable programming in Python.**\n",
        "\n",
        "We will be using **Google Colab** to do the majority of work in the class, since it offers a standard environment regardless of your personal machine. This very file is a Jupyter **notebook** that you can edit and use to run Python code. Its file extension is “.ipynb” for (I)nteractive (Py)thon (N)ote(b)ook.\n",
        "\n",
        "Notebooks are divided into Cells. Some Cells are text (written in Markdown). You won’t need to edit these. The other Cells are executable code and will have `[ ]` to the left of them. After running one of these Cells, a number will appear inside the brackets, indicating the order in which the Cells were run.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **Please make a COPY of this notebook!**\n",
        "\n",
        "Please make a COPY of this notebook when you are getting started; nobody should have edit privileges which means that while you can type and run cells here, **it will NOT save**. Make a copy to your own Colab!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK6AEVjH0_ws"
      },
      "source": [
        "# Part 1: Cloud Environment Setup\n",
        "\n",
        "If you've gotten to this point, you have already successfully logged into Google Colab!  Most likely you'll want to ``Save a Copy in Drive`` for your own use as you edit your code.  We suggest you don't rename the file as you do so.\n",
        "\n",
        "Since this initial homework uses the whole \"big data\" stack, including Apache Spark, we will first need to do some software setup.\n",
        "\n",
        "Generally speaking we will be running command-line options (eg to install software on the host machine) using the `!` operation, and we will be using `pip` to install Python libraries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VnR6DBF1BE6"
      },
      "source": [
        "## 1.1 Installing Spark on Google Colab\n",
        "\n",
        "For big data analysis on a cluster, we'll need to learn to use Apache Spark.  You don't need to fully follow the details here to install Spark on Colab, but you do need to execute the cell!\n",
        "\n",
        "Select it and hit [Shift]-[Enter] to run, or click on the \"play\" triangle to the left."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%set_env SPARK_VERSION=3.4.1\n",
        "%set_env HW_ID=CIS5450_F23_HW0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSKXFI3D-6k2",
        "outputId": "c47f1908-99ea-482c-cbdd-6676cb86901f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: SPARK_VERSION=3.4.1\n",
            "env: HW_ID=CIS5450_F23_HW0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm4lrIFhMVHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd9b11c-1b36-4681-ee94-ad8c2c3ecf58"
      },
      "source": [
        "## Let's install Apache Spark on Colab\n",
        "\n",
        "!wget -nc https://downloads.apache.org/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf spark-$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install findspark\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-\" + os.environ['SPARK_VERSION'] + \"-bin-hadoop3\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘spark-3.4.1-bin-hadoop3.tgz’ already there; not retrieving.\n",
            "\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3Y85s0L9vX5"
      },
      "source": [
        "Good, the software should be installed.\n",
        "\n",
        "Now you need to run three more Cells that configure Jupyter for Apache Spark, set up the environment, and connect to Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N69M3ZWgMYXH"
      },
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SQLContext"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qmxb7iPIMZyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be6ec993-4da6-4555-ce9c-413329dda5f1"
      },
      "source": [
        "try:\n",
        "    if(spark == None):\n",
        "        spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "        sqlContext=SQLContext(spark)\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "    sqlContext=SQLContext(spark)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.4.1-bin-hadoop3/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn3wuswcj3fw"
      },
      "source": [
        "## 1.2 Autograding and the PennGrader\n",
        "\n",
        "<img align=\"right\" src = \"https://imgur.com/rNd3gIg.png\" width= \"200\"/>\n",
        "\n",
        "Next you'll need to set up the PennGrader, which we'll be using throughout the semester to help you with your homeworks.\n",
        "\n",
        "PennGrader is not only **awesome**, but its initial version was built by an equally awesome person: CIS 5450 alumnus, Leo Murri, who later became a TA for the course.  Today Leo works as a data scientist at Amazon!\n",
        "\n",
        "PennGrader was developed to provide students with *instant* feedback on their answer. You can submit your answer and know whether it's right or wrong instantly. We then record your most recent answer in our backend database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFixFKy2kAev"
      },
      "source": [
        "%%capture\n",
        "!pip install penngrader-client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PFKrty7lK5N"
      },
      "source": [
        "Let's try it out! Fill in the cell below with your 8-digit Penn ID and then run the following cell to initialize the grader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcS79C9blJBm"
      },
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 79153661 # YOUR PENN-ID GOES HERE AS AN INTEGER#\n",
        "\n",
        "SECRET = STUDENT_ID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ],
      "metadata": {
        "id": "8ToIBcSTdJ0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from penngrader.grader import *\n",
        "\n",
        "# Import useful libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from string import ascii_letters\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "import requests\n",
        "from lxml import html\n",
        "import math"
      ],
      "metadata": {
        "id": "x62rNEosdQdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd0oMe8glNnA"
      },
      "source": [
        "grader = PennGrader('notebook-config.yaml', os.environ['HW_ID'], STUDENT_ID, SECRET)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVm1kMgrgG6E"
      },
      "source": [
        "# Part 2: Your First CIS 5450 Notebook\n",
        "\n",
        "The rest of the assignment will try to illustrate a few aspects of data analytics...  Don't be concerned if you don't yet know all of the operations, libraries, etc. because that's what we'll be covering soon!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpAxQ5Ac1O5v"
      },
      "source": [
        "## 2.1 A Simple Program to Read from the Web and Invoke Spark\n",
        "\n",
        "The cell below uses the **Pandas** library to read a table from the given web page (the Wikipedia information on films in the year 2010).  The code loads this into a list of **DataFrame**s called `films_2010`.  We then pull the table at index 3, then do some simple **data wrangling** on `top_films` to set up the appropriate types.\n",
        "\n",
        "Select the Cell below and then select the Run button which appeared over the brackets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IG5hBbY97J8"
      },
      "source": [
        "!pip install money-parser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0c5Tsrjhou0"
      },
      "source": [
        "import pandas as pd\n",
        "from money_parser import price_dec\n",
        "\n",
        "def extract_number(x):\n",
        "  items = str(x).split('[')\n",
        "  return round(price_dec(items[0]) / 1000000, 2)\n",
        "\n",
        "films_2023 = pd.read_html('https://en.wikipedia.org/wiki/2023_in_film')\n",
        "\n",
        "top_films = films_2023[2]\n",
        "\n",
        "top_films.set_index('Rank', inplace=True)\n",
        "\n",
        "top_films['Revenue (millions)'] = top_films['Worldwide gross'].apply(extract_number)\n",
        "\n",
        "top_films"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHYYZNCe-eM1"
      },
      "source": [
        "Can we programmatically compute how many entries were scored as top films?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfBjaH1U-hIh"
      },
      "source": [
        "# You can use the Python len() function on a dataframe to figure out how many rows!\n",
        "\n",
        "# TODO: Update dataframe_length with your code here!\n",
        "dataframe_length = len(top_films)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st8grxXxk1cy"
      },
      "source": [
        "# Run this cell to submit to PennGrader!\n",
        "\n",
        "grader.grade(test_case_id = 'length_test', answer = dataframe_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5aP5_h8srJ9"
      },
      "source": [
        "Now we will copy the table over to **Apache Spark**, which is a big data engine capable of processing giant tables.\n",
        "\n",
        "We will **query** the table to get top films from Disney."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUWgHOoBgF6K"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# The 'schema' specifies the column names and data types\n",
        "schema = StructType([StructField('Title', StringType(), nullable=True), \\\n",
        "            StructField('Distributor', StringType(), nullable=False), \\\n",
        "            StructField('Gross', StringType(), nullable=False), \\\n",
        "            StructField('Revenue_M', DecimalType(), nullable=False)])\n",
        "\n",
        "# This loads a Pandas DataFrame into Apache Spark\n",
        "top_films_spark = spark.createDataFrame(top_films, \\\n",
        "                                         schema=schema)\n",
        "\n",
        "# Now use Spark to filter only those rows where the distributor is 'Disney'\n",
        "disney_films = top_films_spark.filter(top_films_spark.Distributor == 'Disney')\n",
        "display(disney_films.collect())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO7JiYf3gzbS"
      },
      "source": [
        "Congratulations, you have just run a very simple Spark program!\n",
        "\n",
        "## 2.2 Something a Little More Fun\n",
        "Running the cell below will create a scatter plot.\n",
        "\n",
        "**Your task is to edit this cell such that:***\n",
        "\n",
        "1. The text (which says “CIS 5450 student”) should be replaced with your full name.\n",
        "2. The number of values sampled should be 500, and you should  change the figure title to match!\n",
        "\n",
        "4. The x-axis should be labeled “Index”.\n",
        "\n",
        "You may run this cell repeatedly to see the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stqXhjIOMbOG"
      },
      "source": [
        "# We’ll be using Matplotlib to plot a visualization\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create a Spark dataset with values 0 thru 499\n",
        "rdd = spark.sparkContext.parallelize(range(500))\n",
        "\n",
        "# TODO: Sample 500 values from the RDD\n",
        "y = np.array(rdd.takeSample(True, 500, 1))\n",
        "# Create an array with the indices\n",
        "x = np.array(range(len(y)))\n",
        "\n",
        "# Create a plot with a caption, X and Y legends, etc\n",
        "x_label = 'Index'\n",
        "y_label = 'Value'\n",
        "student = 'Soohyun Choi'\n",
        "\n",
        "plt.title(str(len(y)) + ' random samples from the RDD')\n",
        "plt.xlabel(x_label)\n",
        "plt.ylabel(y_label)\n",
        "plt.figtext(0.995, 0.01, student, ha='right', va='bottom')\n",
        "# Scatter plot that fits within the box\n",
        "plt.scatter(x, y)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Now fit a trend line to the data and plot it over the scatter plot\n",
        "m, c = np.polyfit(x, y, 1)\n",
        "plt.plot(x, m*x + c)\n",
        "\n",
        "# Save the SVG\n",
        "# plt.savefig('hw0.svg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daKoWpQZg7EJ"
      },
      "source": [
        "The following test cell prints your name and tests whether you followed the directions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXhK2uIFg2BJ"
      },
      "source": [
        "print(\"Your name is:\", student)\n",
        "\n",
        "# Run this cell to submit to PennGrader!\n",
        "grader.grade(test_case_id = 'name_test', answer = student)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: AI over Text\n",
        "\n",
        "You've heard all about Large Language Models. We will use them in some aspects of our study of data importing and wrangling.\n",
        "\n",
        "Next you'll need an API key from OpenAI.\n",
        "\n",
        "If you don't have one already, you can grab one by [signing up](https://platform.openai.com/overview). Then click your account icon on the top right of the screen and select \"View API Keys\". Create an API key.  Fill it in below."
      ],
      "metadata": {
        "id": "P-lbvzjjKEhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%set_env OPENAI_API_KEY = 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"
      ],
      "metadata": {
        "id": "eRW-bcceKWNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "O_D4c3r6KOer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load some documents from Wikipedia."
      ],
      "metadata": {
        "id": "InVrD1DpKe_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir to_index\n",
        "\n",
        "!wget https://en.wikipedia.org/wiki/Large_language_model -O llm.html\n",
        "!wget https://en.wikipedia.org/wiki/Data_integration -O di.html\n",
        "!mv *.html to_index"
      ],
      "metadata": {
        "id": "YvSDqRdhKhj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can index the files (or really, pieces of the files) using LlamaIndex and use this to search."
      ],
      "metadata": {
        "id": "o-29vyPmKmM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "from langchain import OpenAI\n",
        "from llama_index import SimpleDirectoryReader, readers, VectorStoreIndex, LLMPredictor, PromptHelper\n",
        "\n",
        "llm = LLMPredictor(llm=OpenAI(temperature=0,\n",
        "                                        model_name=\"text-davinci-003\",\n",
        "                                        max_tokens=256))\n",
        "prompt_helper = PromptHelper(4096,\n",
        "                              256,\n",
        "                              0.1,\n",
        "                              chunk_size_limit=512)\n",
        "documents = SimpleDirectoryReader('to_index').load_data()\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    llm_predictor=llm,\n",
        "    prompt_helper=prompt_helper,\n",
        "    persistent=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "TrOPzLZ8Kr6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can run the following cell to ask queries against the indexed content in the database. Press the Stop button when you're done."
      ],
      "metadata": {
        "id": "fXs9ukpsKxw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "while True:\n",
        "    query = input(\"What do you want to ask the Chat Bot? \")\n",
        "    response = index.as_query_engine().query(query)\n",
        "    display(Markdown(f\"The Chat Bot says: <b>{response.response}</b>\"))\n"
      ],
      "metadata": {
        "id": "O39X2L-qKvoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzLJBEcl8r2X"
      },
      "source": [
        "# Part 4: Submitting Your Homework\n",
        "\n",
        "First, note that it's easy to \"break\" your notebook by changing something...  So you should *ALWAYS* clear output and re-run your code, just to make sure that hasn't happened.\n",
        "\n",
        "When you are done, select the \"Runtime\" menu at the top of the window. Then, select \"Restart and run all\". Please make sure all cells complete!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvJC6H9k1gYD"
      },
      "source": [
        "## 4.1 Submission to Gradescope\n",
        "\n",
        "Now go to the File menu and choose \"Download .ipynb\" and also \"Download .py\".  Go to [Gradescope](https://www.gradescope.com/courses/576266) and:\n",
        "\n",
        "1. (If you didn't read the above carefully) from \"File\" --> Download *both* .ipynb and .py files\n",
        "1. Rename these downloaded files `homework0.ipynb` and `homework0.py`, respectively\n",
        "1. Sign in using your Penn email address (if you are a SEAS student we recommend using the Google login) and ensure  your class is \"BAN_CIS-5450-001 202330\"\n",
        "1. Select Homework 0\n",
        "1. Upload both files\n",
        "1. PLEASE CHECK THE AUTOGRADER OUTPUT TO ENSURE YOUR SUBMISSION IS PROCESSED CORRECTLY!\n",
        "\n",
        "**NOTE: in future HW, we will apply a penalty if we have to re-upload your submission to Gradescope after the deadline.**.\n",
        "\n",
        "You should be set! Note that this assignment has 8 autograded points and 2 manually graded points! The autograded points will show upon submission, but the manually graded portion will be graded by your TAs after the deadline has passed."
      ]
    }
  ]
}